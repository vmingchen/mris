\section{Related Work} 
\label{sec:related} 
Our work of optimizating performance of a key/value store using hybrid
storage devices is related to (1) hybrid filesystems, (2) multi-tier
storage, and (3) multi-level caching. 

\paragraph{(1) Hybrid Filesystems}

hFS~\cite{eurosys_hfs} is a hybrid filesystem which treats data
differently based on their size and type. Metadata and small files are
stored separately in a log partition like log-structured filesystem;
data blocks of large regular files are stored in a partition in a
FFS-like fashion. Similar to hFS, Conquest~\cite{conquest_tos} uses
battery-backed RAM to hold metadata and small files. Only large files
go to disk. Unlike hFS, UmbrellaFS~\cite{umbrellafs_gos} is a hybrid
stackable filesystem sit bellow VFS but above general filesystems such
as Ext2.  UmbrellaFS is able to use different devices including SSD.
TableFS \cite{tablefs} uses NoSQL store for metadata and small files.
However, its main objective is to improve the performance of a
filesystem using NoSQL store.

Whereas all of them integrate hybrid techniques into the filesystem
layer, our system lies in the application layer which is above the
filesystem layer. It optimizes the operations of an object store,
which provides a different interface from the POSIX filesystem
interface. This is an important difference because the filesystem
interface lacks application level knowledge, which is very useful in
optimizating application performance.

\paragraph{(2) Multi-tier Storage}
%
GTSSL~\cite{socc11chisl} presents an efficient multi-tier key/value
storage architecture, wherein Flash SSD is used for storing top layer
SSTable above the disk. However, workload specific characters, such as
size-tiered property are not exploited. As a follow-up research of
GTSSL, MRIS can integrate nicely with GTSSL. Koltsidas and
Viglas~\cite{vldb_flashup} improved the performance of database by
using both Flash and disk drives. They decided either Flash or disk
should be used in the database buffer manager, which is also unware of
knowledge such as object size. Moreover, their study was based an old
model, which considers Flash write to be 10 times slower than disk
write. This is no longer true, as we see from
Subsection~\ref{sec:drives}, thanks to the development of hardware and
software used in Flash SSD. FAWN~\cite{sosp09fawn} is a distributed
multi-tier key/value store. They used a two-tier architecture of RAM
and Flash SSD, but not Flash SSD and HDD. Most of their strategies are
not applicable here because of the large performance disparity
between RAM and disk. Furthermore, their design for energy saving is
orthognal to our work.

\paragraph{(3) Multi-level Caching}
%
Storage class memory such as Flash fills the gap between DRAM and HDD
in termns of cost, capacity and performance. It can be considered
either as backup for DRAM in the virtual memory layer or cache for HDD
in the block layer. Zhang et al.~\cite{zhang2012multi} and Saxena et
al.~\cite{flashvm} consider using Flash as backup of DRAM for paging,
whereas FlashTier~\cite{eurosys_12_flashtier},
FlashCache~\cite{flashcache} and Bcache~\cite{bcache} use Flash as
block level cache. Our work resides in neither the virtual memory nor
the block layer. It is agnostic to all the above-mentioned techniques.
Moreover, both Zhang et. al.~\cite{zhang2012multi} and Saxena et
al.~\cite{flashvm} tried to reduce cost by replacing a portion of DRAM
with SSD without cost penalty. This objective does not conflict with
design of MRIS, however, it is not in our design concerns.

Forney et al. \cite{Forney2002fast} proposed storage aware caching for
heterogeneous storage systems. They made memory cache aware of the
different replacement costs and partitioned the cache for different
storage devices.  However, their study is set in a different context
which is a network-attached disk system.  They did not consider data
placement among different drives, which is an important strategy of
our study.

%\cite{fast_12_deindirection} also treats metadata and data differently, and
%store them in virtual blocks and physical blocks respectively.

%hFAD \cite{Seltzer09hfad} described an tag-based object store API that
%supports full text search and it focused more on user interface. As
%our work is also able to provide tagging and text searching of
%metadata, we are more focused on workload specific performance
%optimization. Moreover, the design of hFAD is above block level
%storage, whereas our work integrate hybrid block level stroage
%techniques into our storage system.

%\cite{kvworkload_sigmetrics} strong locality metrics, such as keys
%accessed many millions of times a day, do not always suffice for a
%high hit rate; and there is still room for efficiency and hit rate
%improvements in Memcachedâ€™s implementation.  We found that the salient
%size characteristics follow power-law distributions, similar to
%other storage and Web-serving systems

%Why not cache in block level? Because block layer lacks the knowledge of
%objects and files.

%It may even be worthwhile to investigate not caching large objects at
%all, to increase overall hit rates \cite{kvworkload_sigmetrics}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For Emacs:
% Local variables:
% fill-column: 70
% End:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For Vim:
% vim:textwidth=70
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LocalWords:  SMR HDDs drive's SMRs
